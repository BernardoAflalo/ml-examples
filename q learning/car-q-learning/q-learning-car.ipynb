{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 27 time steps with total reward = -9841.500000.\n",
      "Episode 1 finished after 16 time steps with total reward = -9952.000000.\n",
      "Episode 2 finished after 16 time steps with total reward = -9936.000000.\n",
      "Episode 3 finished after 25 time steps with total reward = -9918.500000.\n",
      "Episode 4 finished after 25 time steps with total reward = -9899.000000.\n",
      "Episode 5 finished after 67 time steps with total reward = -9803.500000.\n",
      "Episode 6 finished after 15 time steps with total reward = -9950.000000.\n",
      "Episode 7 finished after 83 time steps with total reward = -9818.000000.\n",
      "Episode 8 finished after 29 time steps with total reward = -9890.000000.\n",
      "Episode 9 finished after 16 time steps with total reward = -9872.500000.\n",
      "Episode 10 finished after 29 time steps with total reward = -9956.500000.\n",
      "Episode 11 finished after 69 time steps with total reward = -9505.000000.\n",
      "Episode 12 finished after 14 time steps with total reward = -9934.500000.\n",
      "Episode 13 finished after 39 time steps with total reward = -9921.000000.\n",
      "Episode 14 finished after 66 time steps with total reward = -9509.000000.\n",
      "Episode 15 finished after 19 time steps with total reward = -9891.000000.\n",
      "Episode 16 finished after 38 time steps with total reward = -9814.000000.\n",
      "Episode 17 finished after 32 time steps with total reward = -9795.500000.\n",
      "Episode 18 finished after 28 time steps with total reward = -9858.000000.\n",
      "Episode 19 finished after 46 time steps with total reward = -9772.500000.\n",
      "Episode 20 finished after 25 time steps with total reward = -9888.000000.\n",
      "Episode 21 finished after 38 time steps with total reward = -9728.500000.\n",
      "Episode 22 finished after 14 time steps with total reward = -9898.000000.\n",
      "Episode 23 finished after 31 time steps with total reward = -9938.000000.\n",
      "Episode 24 finished after 29 time steps with total reward = -9878.500000.\n",
      "Episode 25 finished after 36 time steps with total reward = -9848.500000.\n",
      "Episode 26 finished after 41 time steps with total reward = -9785.000000.\n",
      "Episode 27 finished after 18 time steps with total reward = -9932.500000.\n",
      "Episode 28 finished after 28 time steps with total reward = -9843.000000.\n",
      "Episode 29 finished after 39 time steps with total reward = -9872.500000.\n",
      "Episode 30 finished after 24 time steps with total reward = -9931.500000.\n",
      "Episode 31 finished after 14 time steps with total reward = -9937.500000.\n",
      "Episode 32 finished after 27 time steps with total reward = -9874.500000.\n",
      "Episode 33 finished after 29 time steps with total reward = -9899.500000.\n",
      "Episode 34 finished after 10 time steps with total reward = -9960.500000.\n",
      "Episode 35 finished after 43 time steps with total reward = -9868.500000.\n",
      "Episode 36 finished after 20 time steps with total reward = -9860.000000.\n",
      "Episode 37 finished after 50 time steps with total reward = -9900.000000.\n",
      "Episode 38 finished after 24 time steps with total reward = -9913.500000.\n",
      "Episode 39 finished after 29 time steps with total reward = -9788.500000.\n",
      "Episode 40 finished after 18 time steps with total reward = -9911.500000.\n",
      "Episode 41 finished after 87 time steps with total reward = -9537.500000.\n",
      "Episode 42 finished after 23 time steps with total reward = -9946.500000.\n",
      "Episode 43 finished after 8 time steps with total reward = -9978.500000.\n",
      "Episode 44 finished after 36 time steps with total reward = -9803.500000.\n",
      "Episode 45 finished after 10 time steps with total reward = -9981.000000.\n",
      "Episode 46 finished after 34 time steps with total reward = -9858.500000.\n",
      "Episode 47 finished after 19 time steps with total reward = -9903.000000.\n",
      "Episode 48 finished after 63 time steps with total reward = -9667.500000.\n",
      "Episode 49 finished after 25 time steps with total reward = -9897.500000.\n",
      "Episode 50 finished after 54 time steps with total reward = -9861.500000.\n",
      "Episode 51 finished after 16 time steps with total reward = -9921.000000.\n",
      "Episode 52 finished after 14 time steps with total reward = -9939.500000.\n",
      "Episode 53 finished after 17 time steps with total reward = -9936.000000.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-81131c8df547>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mnum_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mq_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_box\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0msimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-81131c8df547>\u001b[0m in \u001b[0;36msimulate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m# Draw games\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# When episode is done, print reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Data\\github\\ml-examples\\q learning\\car-q-learning\\gym_game\\envs\\custom_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Data\\github\\ml-examples\\q learning\\car-q-learning\\gym_game\\envs\\pygame_2d.py\u001b[0m in \u001b[0;36mview\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reference: https://www.youtube.com/watch?v=ZxXKISVkH6Y\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import gym_game\n",
    "\n",
    "def simulate():\n",
    "    global epsilon, epsilon_decay\n",
    "    for episode in range(MAX_EPISODES):\n",
    "\n",
    "        # Init environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # AI tries up to MAX_TRY times\n",
    "        for t in range(MAX_TRY):\n",
    "\n",
    "            # In the beginning, do random action to learn\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "\n",
    "            # Do action and get result\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Get correspond q value from state, action pair\n",
    "            q_value = q_table[state][action]\n",
    "            best_q = np.max(q_table[next_state])\n",
    "\n",
    "            # Q(state, action) <- (1 - a)Q(state, action) + a(reward + rmaxQ(next state, all actions))\n",
    "            q_table[state][action] = (1 - learning_rate) * q_value + learning_rate * (reward + gamma * best_q)\n",
    "\n",
    "            # Set up for the next iteration\n",
    "            state = next_state\n",
    "\n",
    "            # Draw games\n",
    "            env.render()\n",
    "\n",
    "            # When episode is done, print reward\n",
    "            if done or t >= MAX_TRY - 1:\n",
    "                print(\"Episode %d finished after %i time steps with total reward = %f.\" % (episode, t, total_reward))\n",
    "                break\n",
    "\n",
    "        # exploring rate decay\n",
    "        if epsilon >= 0.005:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"Pygame-v0\")\n",
    "MAX_EPISODES = 999\n",
    "MAX_TRY = 1000\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.999\n",
    "learning_rate = 0.1\n",
    "gamma = 0.6\n",
    "num_box = tuple((env.observation_space.high + np.ones(env.observation_space.shape)).astype(int))\n",
    "q_table = np.zeros(num_box + (env.action_space.n,))\n",
    "simulate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
